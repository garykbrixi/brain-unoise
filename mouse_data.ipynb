{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mouse_data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1rnxfO73aJZyBrnUuKDsE7JInFxhDpFfG",
      "authorship_tag": "ABX9TyMUyS75H5l/JzPLqJttTeeM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "06NiuwR1YvoU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.utils.validation import check_is_fitted, check_array\n",
        "\n",
        "def svd_flip(u, v, u_based_decision=True):\n",
        "    u = u.view(u.size()[0],1)\n",
        "    v = v.view(v.size()[0],1)\n",
        "    if u_based_decision:\n",
        "        # columns of u, rows of v\n",
        "        max_abs_cols = torch.argmax(torch.abs(u), axis=0)\n",
        "        signs = torch.sign(u[list(max_abs_cols), range(u[0].size()[0])])\n",
        "        u *= signs\n",
        "        v *= signs.unsqueeze(1)\n",
        "    else:\n",
        "        # rows of v, columns of u\n",
        "        max_abs_rows = torch.argmax(torch.abs(v), axis=1)\n",
        "        signs = torch.sign(v[range(list(v.size())[0]), max_abs_rows])\n",
        "        u *= signs\n",
        "        v *= signs.unsqueeze(1)\n",
        "    u = u.flatten()\n",
        "    v = v.flatten()\n",
        "    return u, v\n",
        "\n",
        "def _nipals_twoblocks_inner_loop(X, Y, mode=\"A\", max_iter=500, tol=1e-06,\n",
        "                                 norm_y_weights=False):\n",
        "    \"\"\"Inner loop of the iterative NIPALS algorithm.\n",
        "    Provides an alternative to the svd(X'Y); returns the first left and right\n",
        "    singular vectors of X'Y.  See PLS for the meaning of the parameters.  It is\n",
        "    similar to the Power method for determining the eigenvectors and\n",
        "    eigenvalues of a X'Y.\n",
        "    \"\"\"\n",
        "\n",
        "    for col in Y.T:\n",
        "        if torch.any(torch.abs(col) > torch.finfo(torch.double).eps):\n",
        "\n",
        "            y_score = col.detach().view(col.size())\n",
        "\n",
        "            break\n",
        "\n",
        "    # for col in Y.T:\n",
        "    #     if np.any(np.abs(col) > np.finfo(np.double).eps):\n",
        "    #         y_score = col.reshape(len(col), 1)\n",
        "    #         break\n",
        "\n",
        "    x_weights_old = 0\n",
        "    ite = 1\n",
        "    X_pinv = Y_pinv = None\n",
        "    eps = torch.finfo(X.dtype).eps\n",
        "\n",
        "    if mode == \"B\":\n",
        "        # Uses condition from scipy<1.3 in pinv2 which was changed in\n",
        "        # https://github.com/scipy/scipy/pull/10067. In scipy 1.3, the\n",
        "        # condition was changed to depend on the largest singular value\n",
        "        X_t = X.dtype.char.lower()\n",
        "        Y_t = Y.dtype.char.lower()\n",
        "        factor = {'f': 1E3, 'd': 1E6}\n",
        "\n",
        "        cond_X = factor[X_t] * eps\n",
        "        cond_Y = factor[Y_t] * eps\n",
        "\n",
        "    # Inner loop of the Wold algo.\n",
        "    while True:\n",
        "        # 1.1 Update u: the X weights\n",
        "        if mode == \"B\":\n",
        "            if X_pinv is None:\n",
        "                # We use slower pinv2 (same as np.linalg.pinv) for stability\n",
        "                # reasons\n",
        "                X_pinv = torch.pinverse(X, check_finite=False, cond=cond_X)\n",
        "            x_weights = torch.mm(X_pinv, y_score)\n",
        "        else:  # mode\n",
        "            # Mode A regress each X column on y_score\n",
        "\n",
        "            x_weights = torch.mv(X.T, y_score) / torch.dot(y_score.T, y_score)\n",
        "        # If y_score only has zeros x_weights will only have zeros. In\n",
        "        # this case add an epsilon to converge to a more acceptable\n",
        "        # solution\n",
        "        if torch.dot(x_weights.T, x_weights) < eps:\n",
        "            x_weights += eps\n",
        "        # 1.2 Normalize u\n",
        "        x_weights /= torch.sqrt(torch.dot(x_weights.T, x_weights)) + eps\n",
        "        # 1.3 Update x_score: the X latent scores\n",
        "        x_score = torch.mv(X, x_weights)\n",
        "        # 2.1 Update y_weights\n",
        "        if mode == \"B\":\n",
        "            if Y_pinv is None:\n",
        "                # compute once pinv(Y)\n",
        "                Y_pinv = torch.pinverse(Y, check_finite=False, cond=cond_Y)\n",
        "            y_weights = torch.mm(Y_pinv, x_score)\n",
        "        else:\n",
        "            # Mode A regress each Y column on x_score\n",
        "            y_weights = torch.mv(Y.T, x_score) / torch.dot(x_score.T, x_score)\n",
        "        # 2.2 Normalize y_weights\n",
        "        if norm_y_weights:\n",
        "            y_weights /= torch.sqrt(torch.mm(y_weights.T, y_weights)) + eps\n",
        "        # 2.3 Update y_score: the Y latent scores\n",
        "        y_score = torch.mv(Y, y_weights) / (torch.dot(y_weights.T, y_weights) + eps)\n",
        "        # y_score = np.dot(Y, y_weights) / np.dot(y_score.T, y_score) ## BUG\n",
        "        x_weights_diff = x_weights - x_weights_old\n",
        "\n",
        "        if torch.dot(x_weights_diff.T, x_weights_diff) < tol or Y.size()[1] == 1:\n",
        "            break\n",
        "        if ite == max_iter:\n",
        "            warnings.warn('Maximum number of iterations reached',\n",
        "                          ConvergenceWarning)\n",
        "            break\n",
        "        x_weights_old = x_weights\n",
        "        ite += 1\n",
        "    return x_weights, y_weights, ite\n",
        "\n",
        "def _center_scale_xy(X, Y, scale=True):\n",
        "    \"\"\" Center X, Y and scale if the scale parameter==True\n",
        "    Returns\n",
        "    -------\n",
        "        X, Y, x_mean, y_mean, x_std, y_std\n",
        "    \"\"\"\n",
        "    # center\n",
        "    x_mean = torch.mean(X, axis=0)\n",
        "    X -= x_mean\n",
        "    y_mean = torch.mean(Y, axis=0)\n",
        "    Y -= y_mean\n",
        "    # scale\n",
        "    if scale:\n",
        "        x_std = torch.std(X, dim = 0)\n",
        "        x_std[x_std == 0.0] = 1.0\n",
        "        X = X/x_std\n",
        "        y_std = torch.std(Y, dim = 0)\n",
        "        y_std[y_std == 0.0] = 1.0\n",
        "        Y = Y/y_std\n",
        "    else:\n",
        "        x_std = torch.ones(X.size()[1])\n",
        "        y_std = torch.ones(Y.size()[1])\n",
        "    return X, Y, x_mean, y_mean, x_std, y_std\n",
        "\n",
        "class PLS:\n",
        "    def __init__(self, n_components=2, *, scale=True,\n",
        "                 deflation_mode=\"regression\",\n",
        "                 mode=\"A\", algorithm=\"nipals\", norm_y_weights=False,\n",
        "                 max_iter=500, tol=1e-06, copy=True):\n",
        "        self.n_components = n_components\n",
        "        self.deflation_mode = deflation_mode\n",
        "        self.mode = mode\n",
        "        self.norm_y_weights = norm_y_weights\n",
        "        self.scale = scale\n",
        "        self.algorithm = algorithm\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.copy = copy\n",
        "        self.scale = scale\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        \"\"\"Fit model to data.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Training vectors, where n_samples is the number of samples and\n",
        "            n_features is the number of predictors.\n",
        "        Y : array-like of shape (n_samples, n_targets)\n",
        "            Target vectors, where n_samples is the number of samples and\n",
        "            n_targets is the number of response variables.\n",
        "        \"\"\"\n",
        "        X = X.clone().detach()\n",
        "        Y = Y.clone().detach()\n",
        "        if Y.ndim == 1:\n",
        "            Y = Y.reshape(-1, 1)\n",
        "\n",
        "        n = X.size()[0]\n",
        "        p = X.size()[1]\n",
        "        q = Y.size()[1]\n",
        "\n",
        "        # if self.n_components < 1 or self.n_components > p:\n",
        "        #     raise ValueError('Invalid number of components: %d' %\n",
        "        #                      self.n_components)\n",
        "        # if self.algorithm not in (\"svd\", \"nipals\"):\n",
        "        #     raise ValueError(\"Got algorithm %s when only 'svd' \"\n",
        "        #                      \"and 'nipals' are known\" % self.algorithm)\n",
        "        # if self.algorithm == \"svd\" and self.mode == \"B\":\n",
        "        #     raise ValueError('Incompatible configuration: mode B is not '\n",
        "        #                      'implemented with svd algorithm')\n",
        "        # if self.deflation_mode not in [\"canonical\", \"regression\"]:\n",
        "        #     raise ValueError('The deflation mode is unknown')\n",
        "        # Scale (in place)\n",
        "        X, Y, self.x_mean_, self.y_mean_, self.x_std_, self.y_std_ = (\n",
        "            _center_scale_xy(X, Y, self.scale))\n",
        "        # Residuals (deflated) matrices\n",
        "        Xk = X\n",
        "        Yk = Y\n",
        "\n",
        "        # Results matrices\n",
        "        self.x_scores_ = torch.zeros((n, self.n_components))\n",
        "        self.y_scores_ = torch.zeros((n, self.n_components))\n",
        "        self.x_weights_ = torch.zeros((p, self.n_components))\n",
        "        self.y_weights_ = torch.zeros((q, self.n_components))\n",
        "        self.x_loadings_ = torch.zeros((p, self.n_components))\n",
        "        self.y_loadings_ = torch.zeros((q, self.n_components))\n",
        "        self.n_iter_ = []\n",
        "\n",
        "        # NIPALS algo: outer loop, over components\n",
        "        Y_eps = torch.finfo(Yk.dtype).eps\n",
        "\n",
        "        for k in range(self.n_components):\n",
        "            if torch.all(torch.mm(Yk.T, Yk) < torch.finfo(torch.double).eps):\n",
        "                # Yk constant\n",
        "                warnings.warn('Y residual constant at iteration %s' % k)\n",
        "                break\n",
        "            # 1) weights estimation (inner loop)\n",
        "            # -----------------------------------\n",
        "            if self.algorithm == \"nipals\":\n",
        "                # Replace columns that are all close to zero with zeros\n",
        "                Yk_mask = torch.all(torch.abs(Yk) < 10 * Y_eps, axis=0)\n",
        "                Yk[:, Yk_mask] = 0.0\n",
        "\n",
        "                x_weights, y_weights, n_iter_ = \\\n",
        "                    _nipals_twoblocks_inner_loop(\n",
        "                        X=Xk, Y=Yk, mode=self.mode, max_iter=self.max_iter,\n",
        "                        tol=self.tol, norm_y_weights=self.norm_y_weights)\n",
        "                self.n_iter_.append(n_iter_)\n",
        "\n",
        "            elif self.algorithm == \"svd\":\n",
        "                x_weights, y_weights = _svd_cross_product(X=Xk, Y=Yk)\n",
        "            # Forces sign stability of x_weights and y_weights\n",
        "            # Sign undeterminacy issue from svd if algorithm == \"svd\"\n",
        "            # and from platform dependent computation if algorithm == 'nipals'\n",
        "\n",
        "            # POTENTIAL TO DO\n",
        "\n",
        "            x_weights, y_weights = svd_flip(x_weights, y_weights.T)\n",
        "            y_weights = y_weights.T\n",
        "            # columns of u, rows of v\n",
        "\n",
        "            # compute scores\n",
        "            x_scores = torch.mv(Xk, x_weights)\n",
        "\n",
        "            if self.norm_y_weights:\n",
        "                y_ss = 1\n",
        "            else:\n",
        "                y_ss = torch.dot(y_weights.T, y_weights)\n",
        "\n",
        "            y_scores = torch.mv(Yk, y_weights) / y_ss\n",
        "\n",
        "            # test for null variance\n",
        "            if torch.dot(x_scores.T, x_scores) < torch.finfo(torch.double).eps:\n",
        "                warnings.warn('X scores are null at iteration %s' % k)\n",
        "                break\n",
        "            # 2) Deflation (in place)\n",
        "            # ----------------------\n",
        "            # Possible memory footprint reduction may done here: in order to\n",
        "            # avoid the allocation of a data chunk for the rank-one\n",
        "            # approximations matrix which is then subtracted to Xk, we suggest\n",
        "            # to perform a column-wise deflation.\n",
        "            #\n",
        "            # - regress Xk's on x_score\n",
        "\n",
        "            x_loadings = torch.mv(Xk.T, x_scores) / torch.dot(x_scores.T, x_scores)\n",
        "            # - subtract rank-one approximations to obtain remainder matrix\n",
        "            Xk -= x_scores[:, None] * x_loadings.T\n",
        "            if self.deflation_mode == \"canonical\":\n",
        "                # - regress Yk's on y_score, then subtract rank-one approx.\n",
        "                y_loadings = (torch.mv(Yk.T, y_scores)\n",
        "                              / torch.dot(y_scores.T, y_scores))\n",
        "                Yk -= y_scores[:, None] * y_loadings.T\n",
        "            if self.deflation_mode == \"regression\":\n",
        "                # - regress Yk's on x_score, then subtract rank-one approx.\n",
        "                y_loadings = (torch.mv(Yk.T, x_scores)\n",
        "                              / torch.dot(x_scores.T, x_scores))\n",
        "                Yk -= x_scores[:, None] * y_loadings.T\n",
        "            # 3) Store weights, scores and loadings # Notation:\n",
        "            self.x_scores_[:, k] = x_scores.view(-1)  # T\n",
        "            self.y_scores_[:, k] = y_scores.view(-1)  # U\n",
        "            self.x_weights_[:, k] = x_weights.view(-1)  # W\n",
        "            self.y_weights_[:, k] = y_weights.view(-1)  # C\n",
        "            self.x_loadings_[:, k] = x_loadings.view(-1)  # P\n",
        "            self.y_loadings_[:, k] = y_loadings.view(-1)  # Q\n",
        "\n",
        "        # Such that: X = TP' + Err and Y = UQ' + Err\n",
        "\n",
        "        # 4) rotations from input space to transformed space (scores)\n",
        "        # T = X W(P'W)^-1 = XW* (W* : p x k matrix)\n",
        "        # U = Y C(Q'C)^-1 = YC* (W* : q x k matrix)\n",
        "        self.x_rotations_ = torch.mm(\n",
        "            self.x_weights_,\n",
        "            torch.pinverse(torch.mm(self.x_loadings_.T, self.x_weights_)))\n",
        "        if Y.size()[1] > 1:\n",
        "            self.y_rotations_ = torch.mm(\n",
        "                self.y_weights_,\n",
        "                torch.pinverse(torch.mm(self.y_loadings_.T, self.y_weights_)))\n",
        "        else:\n",
        "            self.y_rotations_ = torch.ones(1)\n",
        "\n",
        "        if True or self.deflation_mode == \"regression\":\n",
        "            # FIXME what's with the if?\n",
        "            # Estimate regression coefficient\n",
        "            # Regress Y on T\n",
        "            # Y = TQ' + Err,\n",
        "            # Then express in function of X\n",
        "            # Y = X W(P'W)^-1Q' + Err = XB + Err\n",
        "            # => B = W*Q' (p x q)\n",
        "\n",
        "            self.coef_ = torch.mm(self.x_rotations_, self.y_loadings_.T)\n",
        "            self.coef_ = self.coef_.to(\"cuda\")\n",
        "            self.y_std_ = self.y_std_.to(\"cuda\")\n",
        "            # self.coef_ = torch.mv(self.coef_, self.y_std_)\n",
        "            self.coef_ = self.coef_[:, None] * self.y_std_\n",
        "            self.coef_ = self.coef_[:,0,:]\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, Y=None, copy=True):\n",
        "        \"\"\"Apply the dimension reduction learned on the train data.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Training vectors, where n_samples is the number of samples and\n",
        "            n_features is the number of predictors.\n",
        "        Y : array-like of shape (n_samples, n_targets)\n",
        "            Target vectors, where n_samples is the number of samples and\n",
        "            n_targets is the number of response variables.\n",
        "        copy : boolean, default True\n",
        "            Whether to copy X and Y, or perform in-place normalization.\n",
        "        Returns\n",
        "        -------\n",
        "        x_scores if Y is not given, (x_scores, y_scores) otherwise.\n",
        "        \"\"\"\n",
        "        check_is_fitted(self)\n",
        "        X = check_array(X, copy=copy, dtype=FLOAT_DTYPES)\n",
        "        # Normalize\n",
        "        X -= self.x_mean_\n",
        "        X /= self.x_std_\n",
        "        # Apply rotation\n",
        "        x_scores = torch.mm(X, self.x_rotations_)\n",
        "        if Y is not None:\n",
        "            Y = check_array(Y, ensure_2d=False, copy=copy, dtype=FLOAT_DTYPES)\n",
        "            if Y.ndim == 1:\n",
        "                Y = Y.reshape(-1, 1)\n",
        "            Y -= self.y_mean_\n",
        "            Y /= self.y_std_\n",
        "            y_scores = torch.mm(Y, self.y_rotations_)\n",
        "            return x_scores, y_scores\n",
        "\n",
        "        return x_scores\n",
        "\n",
        "    def inverse_transform(self, X):\n",
        "        \"\"\"Transform data back to its original space.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape (n_samples, n_components)\n",
        "            New data, where n_samples is the number of samples\n",
        "            and n_components is the number of pls components.\n",
        "        Returns\n",
        "        -------\n",
        "        x_reconstructed : array-like of shape (n_samples, n_features)\n",
        "        Notes\n",
        "        -----\n",
        "        This transformation will only be exact if n_components=n_features\n",
        "        \"\"\"\n",
        "        check_is_fitted(self)\n",
        "        X = check_array(X, dtype=FLOAT_DTYPES)\n",
        "        # From pls space to original space\n",
        "        X_reconstructed = torch.matmul(X, self.x_loadings_.T)\n",
        "\n",
        "        # Denormalize\n",
        "        X_reconstructed *= self.x_std_\n",
        "        X_reconstructed += self.x_mean_\n",
        "        return X_reconstructed\n",
        "\n",
        "    def predict(self, X, copy=True):\n",
        "        \"\"\"Apply the dimension reduction learned on the train data.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Training vectors, where n_samples is the number of samples and\n",
        "            n_features is the number of predictors.\n",
        "        copy : boolean, default True\n",
        "            Whether to copy X and Y, or perform in-place normalization.\n",
        "        Notes\n",
        "        -----\n",
        "        This call requires the estimation of a p x q matrix, which may\n",
        "        be an issue in high dimensional space.\n",
        "        \"\"\"\n",
        "        # TODO: check fitted and check array\n",
        "        # check_is_fitted(self)\n",
        "        # X = check_array(X, copy=copy, dtype=FLOAT_DTYPES)\n",
        "        # Normalize\n",
        "        X -= self.x_mean_\n",
        "        X /= self.x_std_\n",
        "        # print(X[:, None] * self.coef_ + self.y_mean_)\n",
        "        # print(torch.mv(X, self.coef_) + self.y_mean_)\n",
        "        Ypred = torch.mm(X, self.coef_)\n",
        "        return Ypred + self.y_mean_\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        \"\"\"Learn and apply the dimension reduction on the train data.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Training vectors, where n_samples is the number of samples and\n",
        "            n_features is the number of predictors.\n",
        "        y : array-like of shape (n_samples, n_targets)\n",
        "            Target vectors, where n_samples is the number of samples and\n",
        "            n_targets is the number of response variables.\n",
        "        Returns\n",
        "        -------\n",
        "        x_scores if Y is not given, (x_scores, y_scores) otherwise.\n",
        "        \"\"\"\n",
        "        return self.fit(X, y).transform(X, y)\n",
        "\n",
        "    def _more_tags(self):\n",
        "        return {'poor_score': True}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-stW8SoV-Mfu",
        "colab_type": "code",
        "outputId": "ea9f5769-29f4-451b-e007-aa61243e6da2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "# from fastai.torch_core import flatten_model\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "from fastai.vision import *\n",
        "from fastai.metrics import error_rate\n",
        "from PIL import Image\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "import pickle\n",
        "\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "# prepare data\n",
        "PATH = \"drive/My Drive/neuro140/\"\n",
        "infile = open(PATH+'/mouse_brain_data_sample.pkl','rb')\n",
        "mouse_pickle = pickle.load(infile)\n",
        "VISam = mouse_pickle['VISam']\n",
        "print(VISam[118])\n",
        "VIspm = mouse_pickle['VISpm']\n",
        "\n",
        "img_array = np.load(PATH+'/stimulus_set.npy')\n",
        "\n",
        "create_images = False\n",
        "dictlist = []\n",
        "img_names = []\n",
        "for i in range(119):\n",
        "  pngfilename = str(i)+str('.jpg')\n",
        "  if (create_images):\n",
        "    im = Image.fromarray(img_array[i])\n",
        "    im.save(PATH + 'images/' + pngfilename)\n",
        "  row = {'ImageName': pngfilename, 'ImageIndex': i}\n",
        "  img_names.append(pngfilename)\n",
        "  dictlist.append(row)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.120102 0.020811 0.       0.018055 ... 0.008298 0.004659 0.       0.      ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpitqrZ-r4Yz",
        "colab_type": "code",
        "outputId": "dc4f77e6-d8bd-4add-a86a-fd4fab3939a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "from fastai.torch_core import flatten_model\n",
        "\n",
        "preprocessing = transforms.Compose([\n",
        "    transforms.Resize((224,224)), \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "class SaveFeatures():\n",
        "    def __init__(self, module):\n",
        "        self.hook = module.register_forward_hook(self.hook_fn)\n",
        "    def hook_fn(self, module, input, output):\n",
        "        self.features = output.clone().detach().requires_grad_(True).cuda()\n",
        "    def close(self):\n",
        "        self.hook.remove()\n",
        "        \n",
        "def get_layer_names(layers):\n",
        "    layer_names = []\n",
        "    for layer in layers:\n",
        "        layer_name = str(layer).split('(')[0]\n",
        "        layer_names.append(layer_name + '-' + str(sum(layer_name in string for string in layer_names) + 1))\n",
        "    return layer_names\n",
        "\n",
        "def get_activations(model, img, layers, target_layer):\n",
        "    img_tensor = preprocessing(img).unsqueeze(0)\n",
        "    activations = SaveFeatures(layers[target_layer])\n",
        "    model(img_tensor)\n",
        "    activations.close()\n",
        "    return activations.features.detach().cpu().numpy().squeeze()\n",
        "\n",
        "# def flatten_batch(features):\n",
        "#     return features.view(features.size()[0], torch.prod(torch.tensor(features.size()[1:])).item())\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "# model.eval()\n",
        "\n",
        "layers = flatten_model(model)\n",
        "layer_names = get_layer_names(layers)\n",
        "print(layer_names)\n",
        "\n",
        "path = Path(PATH + 'images/')\n",
        "\n",
        "layer_wise_activation = False\n",
        "if(layer_wise_activation):\n",
        "  classifier_rdms = {}\n",
        "  for layer_index, layer in enumerate(tqdm(layers[:-2])):\n",
        "      image_activations = []\n",
        "      for image in img_names:\n",
        "          image_array = Image.open(path/image)\n",
        "          print(get_activations(model, image_array, layers, layer_index).flatten())\n",
        "          image_activations.append(get_activations(model, image_array, layers, layer_index).flatten())\n",
        "      layer_features = np.stack(image_activations)\n",
        "  #save file\n",
        "  with open(PATH + 'classifier_dict.pickle', 'wb') as f:\n",
        "    pickle.dump(classifier_rdms, f)\n",
        "\n",
        "image_wise_activations = False\n",
        "if(image_wise_activations):\n",
        "  image_activations = []\n",
        "  for image in img_names:\n",
        "    image_array = Image.open(path/image)\n",
        "    layer_activations = []\n",
        "    for layer_index, layer in enumerate(tqdm(layers[:-2])):\n",
        "      layer_activations.append(get_activations(model, image_array, layers, layer_index))\n",
        "    image_activations.append(layer_activations)\n",
        "  #save file\n",
        "  x = np.array(image_activations)\n",
        "  np.save(str(PATH + 'image_activations.npy'), x, allow_pickle=True)\n",
        "\n",
        "# scaler = transforms.Scale((224, 224))df\n",
        "# normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "#                                  std=[0.229, 0.224, 0.225])\n",
        "# to_tensor = transforms.ToTensor()\n",
        "\n",
        "# print(model.named_modules())\n",
        "\n",
        "# layer1 = model._modules.get('Conv2d-1')\n",
        "    \n",
        "# print(layer1)\n",
        "# img_vectors = []\n",
        "# for i in range(len(img_names)):\n",
        "#   img_vectors.append([])\n",
        "#   img = str(PATH + 'images/' + img_names[i])\n",
        "#   for j in layer_names:\n",
        "#     print(layer1)\n",
        "#     layer1 = model._modules.get(layer1)\n",
        "#     img_vectors[i].append(get_vector(img, layer1))\n",
        "\n",
        "# print(img_vectors['0.jpg'].size())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Conv2d-1', 'BatchNorm2d-1', 'ReLU-1', 'MaxPool2d-1', 'Conv2d-2', 'BatchNorm2d-2', 'ReLU-2', 'Conv2d-3', 'BatchNorm2d-3', 'Conv2d-4', 'BatchNorm2d-4', 'ReLU-3', 'Conv2d-5', 'BatchNorm2d-5', 'Conv2d-6', 'BatchNorm2d-6', 'ReLU-4', 'Conv2d-7', 'BatchNorm2d-7', 'Conv2d-8', 'BatchNorm2d-8', 'Conv2d-9', 'BatchNorm2d-9', 'ReLU-5', 'Conv2d-10', 'BatchNorm2d-10', 'Conv2d-11', 'BatchNorm2d-11', 'ReLU-6', 'Conv2d-12', 'BatchNorm2d-12', 'Conv2d-13', 'BatchNorm2d-13', 'Conv2d-14', 'BatchNorm2d-14', 'ReLU-7', 'Conv2d-15', 'BatchNorm2d-15', 'Conv2d-16', 'BatchNorm2d-16', 'ReLU-8', 'Conv2d-17', 'BatchNorm2d-17', 'Conv2d-18', 'BatchNorm2d-18', 'Conv2d-19', 'BatchNorm2d-19', 'ReLU-9', 'Conv2d-20', 'BatchNorm2d-20', 'AdaptiveAvgPool2d-1', 'Linear-1']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2DP-l-nB-nh",
        "colab_type": "code",
        "outputId": "edfa29c1-639e-41b5-e272-a3d9af3e8bf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# image_activations = np.load(str(PATH+'image_activations.npy'), allow_pickle=True)\n",
        "\n",
        "# for row in img_activations:\n",
        "#   print(column.shape)\n",
        "\n",
        "# print(VISam[0])\n",
        "# X = np.transpose(VISam)[0].shape\n",
        "\n",
        "layer_3 = np.zeros((119, 200704))\n",
        "for i in range(len(image_activations)):\n",
        "  layer_3[i] = image_activations[i][3].flatten()\n",
        "\n",
        "print(layer_3.shape)\n",
        "print(VISam.shape)\n",
        "# neuron wise activations\n",
        "VISam_neuron_wise_pred = []\n",
        "for i in range(VISam.shape[0]):\n",
        "  x = layer_3\n",
        "  y = np.transpose(VISam)[i]\n",
        "  y = y.reshape(-1, 1)\n",
        "  # check_array(y, dtype=np.float64, ensure_2d=False)\n",
        "  print(x.shape)\n",
        "  print(y.shape)\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "  print(X_test)\n",
        "  modl = PLSRegression(n_components=25)\n",
        "  modl = modl.fit(x, y)\n",
        "  Y_pred = modl.predict(X_test)\n",
        "  VISam_neuron_wise_pred.append(Y_pred)\n",
        "  print(Y_pred)\n",
        "\n",
        "# # R_X = torch.tensor(X_train).type(torch.cuda.FloatTensor)\n",
        "# # R_Y = torch.tensor(Y_train).type(torch.cuda.FloatTensor)\n",
        "# # R_Xtest = torch.tensor(X_test).type(torch.cuda.FloatTensor)ççç\n",
        "# # modl = PLS(n_components=2).fit(R_X, R_Y)\n",
        "# # Y_pred = modl.predict(R_Xtest)\n",
        "# # Y_pred = Y_pred.cpu().numpy()\n",
        "\n",
        "# # print(np.average(np.corrcoef(Y_pred, Y_test)))\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(119, 200704)\n",
            "(119, 221)\n",
            "(119, 200704)\n",
            "(119, 1)\n",
            "[[0.144833 0.167497 0.186572 0.186572 ... 0.92448  0.886856 0.802933 0.593327]\n",
            " [0.422123 0.422123 0.864352 1.13177  ... 0.327513 0.327513 0.616996 0.616996]\n",
            " [1.497047 1.649969 1.968521 1.975232 ... 1.073336 1.230291 1.031057 1.675296]\n",
            " [2.28661  2.28661  2.359669 2.380687 ... 0.405165 0.212393 0.399554 0.399554]\n",
            " ...\n",
            " [1.234699 1.234699 1.448261 1.448261 ... 0.676144 0.686929 0.636533 0.846747]\n",
            " [0.196176 0.196176 0.158061 0.120332 ... 0.357042 0.248835 0.248835 0.23788 ]\n",
            " [0.175792 0.079074 0.187394 0.187394 ... 0.238883 0.238883 0.235768 0.244489]\n",
            " [1.372641 1.503724 1.534388 1.534388 ... 1.005076 1.246447 1.259979 1.614688]]\n",
            "[[ 8.877196e-03]\n",
            " [ 2.486467e-03]\n",
            " [ 9.368593e-02]\n",
            " [ 7.341749e-10]\n",
            " [ 2.133432e-02]\n",
            " [ 8.296191e-02]\n",
            " [ 1.401422e-09]\n",
            " [ 6.056975e-03]\n",
            " [ 8.654715e-03]\n",
            " [ 2.715355e-02]\n",
            " [ 1.796429e-02]\n",
            " [ 2.202499e-03]\n",
            " [ 1.319784e-02]\n",
            " [ 3.728627e-03]\n",
            " [ 2.055124e-02]\n",
            " [ 6.709184e-03]\n",
            " [ 3.975109e-03]\n",
            " [ 8.755714e-03]\n",
            " [ 7.766039e-03]\n",
            " [-3.588972e-10]\n",
            " [-9.834597e-10]\n",
            " [ 3.467618e-02]\n",
            " [ 6.085024e-03]\n",
            " [ 3.146477e-10]]\n",
            "(119, 200704)\n",
            "(119, 1)\n",
            "[[0.144833 0.167497 0.186572 0.186572 ... 0.92448  0.886856 0.802933 0.593327]\n",
            " [0.422123 0.422123 0.864352 1.13177  ... 0.327513 0.327513 0.616996 0.616996]\n",
            " [1.497047 1.649969 1.968521 1.975232 ... 1.073336 1.230291 1.031057 1.675296]\n",
            " [2.28661  2.28661  2.359669 2.380687 ... 0.405165 0.212393 0.399554 0.399554]\n",
            " ...\n",
            " [1.234699 1.234699 1.448261 1.448261 ... 0.676144 0.686929 0.636533 0.846747]\n",
            " [0.196176 0.196176 0.158061 0.120332 ... 0.357042 0.248835 0.248835 0.23788 ]\n",
            " [0.175792 0.079074 0.187394 0.187394 ... 0.238883 0.238883 0.235768 0.244489]\n",
            " [1.372641 1.503724 1.534388 1.534388 ... 1.005076 1.246447 1.259979 1.614688]]\n",
            "[[0.019649]\n",
            " [0.032948]\n",
            " [0.040592]\n",
            " [0.015702]\n",
            " [0.003367]\n",
            " [0.016789]\n",
            " [0.011012]\n",
            " [0.030551]\n",
            " [0.021235]\n",
            " [0.021173]\n",
            " [0.00412 ]\n",
            " [0.033618]\n",
            " [0.044546]\n",
            " [0.02138 ]\n",
            " [0.023709]\n",
            " [0.006858]\n",
            " [0.014568]\n",
            " [0.003833]\n",
            " [0.057481]\n",
            " [0.002767]\n",
            " [0.021003]\n",
            " [0.013441]\n",
            " [0.026017]\n",
            " [0.001915]]\n",
            "(119, 200704)\n",
            "(119, 1)\n",
            "[[0.144833 0.167497 0.186572 0.186572 ... 0.92448  0.886856 0.802933 0.593327]\n",
            " [0.422123 0.422123 0.864352 1.13177  ... 0.327513 0.327513 0.616996 0.616996]\n",
            " [1.497047 1.649969 1.968521 1.975232 ... 1.073336 1.230291 1.031057 1.675296]\n",
            " [2.28661  2.28661  2.359669 2.380687 ... 0.405165 0.212393 0.399554 0.399554]\n",
            " ...\n",
            " [1.234699 1.234699 1.448261 1.448261 ... 0.676144 0.686929 0.636533 0.846747]\n",
            " [0.196176 0.196176 0.158061 0.120332 ... 0.357042 0.248835 0.248835 0.23788 ]\n",
            " [0.175792 0.079074 0.187394 0.187394 ... 0.238883 0.238883 0.235768 0.244489]\n",
            " [1.372641 1.503724 1.534388 1.534388 ... 1.005076 1.246447 1.259979 1.614688]]\n",
            "[[ 1.320086e-03]\n",
            " [ 1.965742e-11]\n",
            " [ 4.014326e-03]\n",
            " [ 1.128888e-03]\n",
            " [-1.176950e-11]\n",
            " [ 1.285481e-03]\n",
            " [ 9.228819e-11]\n",
            " [ 2.510635e-03]\n",
            " [ 7.603823e-11]\n",
            " [ 6.356147e-03]\n",
            " [ 2.281687e-10]\n",
            " [ 1.089849e-03]\n",
            " [ 2.231550e-03]\n",
            " [-5.407695e-11]\n",
            " [ 2.800476e-03]\n",
            " [ 1.205269e-03]\n",
            " [-3.487119e-11]\n",
            " [ 1.414586e-03]\n",
            " [-9.653143e-11]\n",
            " [ 1.539259e-03]\n",
            " [ 2.738775e-02]\n",
            " [ 3.215799e-03]\n",
            " [ 2.336737e-03]\n",
            " [ 1.399911e-02]]\n",
            "(119, 200704)\n",
            "(119, 1)\n",
            "[[0.144833 0.167497 0.186572 0.186572 ... 0.92448  0.886856 0.802933 0.593327]\n",
            " [0.422123 0.422123 0.864352 1.13177  ... 0.327513 0.327513 0.616996 0.616996]\n",
            " [1.497047 1.649969 1.968521 1.975232 ... 1.073336 1.230291 1.031057 1.675296]\n",
            " [2.28661  2.28661  2.359669 2.380687 ... 0.405165 0.212393 0.399554 0.399554]\n",
            " ...\n",
            " [1.234699 1.234699 1.448261 1.448261 ... 0.676144 0.686929 0.636533 0.846747]\n",
            " [0.196176 0.196176 0.158061 0.120332 ... 0.357042 0.248835 0.248835 0.23788 ]\n",
            " [0.175792 0.079074 0.187394 0.187394 ... 0.238883 0.238883 0.235768 0.244489]\n",
            " [1.372641 1.503724 1.534388 1.534388 ... 1.005076 1.246447 1.259979 1.614688]]\n",
            "[[ 2.190334e-03]\n",
            " [ 1.204770e-03]\n",
            " [ 3.451912e-02]\n",
            " [ 2.381685e-02]\n",
            " [-1.572304e-09]\n",
            " [ 1.363373e-02]\n",
            " [ 1.186368e-03]\n",
            " [ 1.271549e-03]\n",
            " [ 2.602314e-02]\n",
            " [-3.144809e-10]\n",
            " [ 9.772791e-01]\n",
            " [ 1.213693e-03]\n",
            " [ 3.343237e-03]\n",
            " [ 1.413677e-02]\n",
            " [ 2.591161e-02]\n",
            " [ 5.878273e-02]\n",
            " [ 1.722407e-02]\n",
            " [ 7.605978e-03]\n",
            " [-1.003983e-09]\n",
            " [ 9.386557e-03]\n",
            " [ 3.723731e-03]\n",
            " [ 6.647423e-03]\n",
            " [-1.566182e-09]\n",
            " [ 8.447332e-02]]\n",
            "(119, 200704)\n",
            "(119, 1)\n",
            "[[0.144833 0.167497 0.186572 0.186572 ... 0.92448  0.886856 0.802933 0.593327]\n",
            " [0.422123 0.422123 0.864352 1.13177  ... 0.327513 0.327513 0.616996 0.616996]\n",
            " [1.497047 1.649969 1.968521 1.975232 ... 1.073336 1.230291 1.031057 1.675296]\n",
            " [2.28661  2.28661  2.359669 2.380687 ... 0.405165 0.212393 0.399554 0.399554]\n",
            " ...\n",
            " [1.234699 1.234699 1.448261 1.448261 ... 0.676144 0.686929 0.636533 0.846747]\n",
            " [0.196176 0.196176 0.158061 0.120332 ... 0.357042 0.248835 0.248835 0.23788 ]\n",
            " [0.175792 0.079074 0.187394 0.187394 ... 0.238883 0.238883 0.235768 0.244489]\n",
            " [1.372641 1.503724 1.534388 1.534388 ... 1.005076 1.246447 1.259979 1.614688]]\n",
            "[[0.117013]\n",
            " [0.082069]\n",
            " [0.091511]\n",
            " [0.083527]\n",
            " [0.113718]\n",
            " [0.113547]\n",
            " [0.131007]\n",
            " [0.056931]\n",
            " [0.235993]\n",
            " [0.113086]\n",
            " [2.844588]\n",
            " [0.121046]\n",
            " [0.08623 ]\n",
            " [0.113755]\n",
            " [0.231905]\n",
            " [0.187042]\n",
            " [0.215066]\n",
            " [0.165989]\n",
            " [0.089699]\n",
            " [0.080262]\n",
            " [0.719133]\n",
            " [0.402931]\n",
            " [0.090573]\n",
            " [0.400009]]\n",
            "(119, 200704)\n",
            "(119, 1)\n",
            "[[0.144833 0.167497 0.186572 0.186572 ... 0.92448  0.886856 0.802933 0.593327]\n",
            " [0.422123 0.422123 0.864352 1.13177  ... 0.327513 0.327513 0.616996 0.616996]\n",
            " [1.497047 1.649969 1.968521 1.975232 ... 1.073336 1.230291 1.031057 1.675296]\n",
            " [2.28661  2.28661  2.359669 2.380687 ... 0.405165 0.212393 0.399554 0.399554]\n",
            " ...\n",
            " [1.234699 1.234699 1.448261 1.448261 ... 0.676144 0.686929 0.636533 0.846747]\n",
            " [0.196176 0.196176 0.158061 0.120332 ... 0.357042 0.248835 0.248835 0.23788 ]\n",
            " [0.175792 0.079074 0.187394 0.187394 ... 0.238883 0.238883 0.235768 0.244489]\n",
            " [1.372641 1.503724 1.534388 1.534388 ... 1.005076 1.246447 1.259979 1.614688]]\n",
            "[[ 6.584654e-03]\n",
            " [ 1.557030e-10]\n",
            " [ 4.682147e-10]\n",
            " [ 1.531843e-02]\n",
            " [ 4.963054e-03]\n",
            " [ 2.741653e-02]\n",
            " [ 9.579265e-03]\n",
            " [ 3.289627e-03]\n",
            " [ 2.509121e-03]\n",
            " [ 4.824908e-03]\n",
            " [-5.368021e-10]\n",
            " [ 8.080410e-03]\n",
            " [ 3.624076e-03]\n",
            " [ 3.828089e-03]\n",
            " [ 1.915667e-02]\n",
            " [ 4.329257e-02]\n",
            " [ 3.360760e-10]\n",
            " [ 3.834648e-03]\n",
            " [ 1.170484e-02]\n",
            " [ 9.147912e-03]\n",
            " [-5.521529e-10]\n",
            " [ 1.747183e-03]\n",
            " [ 7.769713e-03]\n",
            " [ 1.061587e-02]]\n",
            "(119, 200704)\n",
            "(119, 1)\n",
            "[[0.144833 0.167497 0.186572 0.186572 ... 0.92448  0.886856 0.802933 0.593327]\n",
            " [0.422123 0.422123 0.864352 1.13177  ... 0.327513 0.327513 0.616996 0.616996]\n",
            " [1.497047 1.649969 1.968521 1.975232 ... 1.073336 1.230291 1.031057 1.675296]\n",
            " [2.28661  2.28661  2.359669 2.380687 ... 0.405165 0.212393 0.399554 0.399554]\n",
            " ...\n",
            " [1.234699 1.234699 1.448261 1.448261 ... 0.676144 0.686929 0.636533 0.846747]\n",
            " [0.196176 0.196176 0.158061 0.120332 ... 0.357042 0.248835 0.248835 0.23788 ]\n",
            " [0.175792 0.079074 0.187394 0.187394 ... 0.238883 0.238883 0.235768 0.244489]\n",
            " [1.372641 1.503724 1.534388 1.534388 ... 1.005076 1.246447 1.259979 1.614688]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-9b4a1cd0148b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mmodl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPLSRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mmodl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mVISam_neuron_wise_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/cross_decomposition/_pls.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0mx_loadings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_scores\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;31m# - subtract rank-one approximations to obtain remainder matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m             \u001b[0mXk\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_loadings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeflation_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"canonical\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0;31m# - regress Yk's on y_score, then subtract rank-one approx.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DO8UdbRW-OXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cov(x, rowvar = False):\n",
        "  if not rowvar:\n",
        "    x = x.t()\n",
        "  f = 1.0 / (x.size(1) - 1)\n",
        "  x -= torch.mean(x, dim=1, keepdim=True)\n",
        "  xt = x.t()\n",
        "  c = f * x.matmul(xt).squeeze()\n",
        "  return c\n",
        "\n",
        "\n",
        "def PCA(data, dims = 2):\n",
        "  \"\"\"\n",
        "  based on numpy function:\n",
        "  https://stackoverflow.com/questions/13224362/principal-component-analysis-pca-in-python\n",
        "  \"\"\"\n",
        "  n = data.shape\n",
        "  mx = torch.mean(data, dim=0)\n",
        "\n",
        "  x = data - mx\n",
        "  R = cov(x, False)\n",
        "\n",
        "  evals, evecs = torch.symeig(R_t, eigenvectors = True, upper = True)\n",
        "  \n",
        "  idx = torch.argsort(evals, descending = True)\n",
        "  evecs = evecs[:,idx]\n",
        "  evals = evals[idx]\n",
        "\n",
        "  evecs = evecs[:, :dims]\n",
        "\n",
        "  return torch.mm(evecs.T, data.T).T, evals, evecs\n",
        "\n",
        "\n",
        "from sklearn.decomposition import PCA as PCA_sk\n",
        "\n",
        "pca = PCA_sk(n_components=5)\n",
        "pca.fit(VISam)\n",
        "Sam_c = pca.components_\n",
        "print(Sam_c)\n",
        "\n",
        "pca = PCA_sk(n_components=5)\n",
        "pca.fit(VIspm)\n",
        "Spm_c = pca.components_\n",
        "print(Spm_c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhqulAaCZdqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_ids = {'alexnet': models.alexnet, 'vgg16': models.vgg16_bn, 'vgg19': models.vgg19_bn, 'resnet18': models.resnet18, 'resnet34': models.resnet34, 'resnet50': models.resnet50, 'resnet152': models.resnet152}\n",
        "\n",
        "# experiment = {'train_data': 'SimpleShapes',\n",
        "#               'test_data': 'SimpleShapes',\n",
        "#               'subset': 'Stack_3',\n",
        "#               'model': 'alexnet', \n",
        "#               'finetuning': False, \n",
        "#               'trainCycles': 3, \n",
        "#               'trainSetSize': 90,\n",
        "#               'valSetSize': 28}\n",
        "\n",
        "# from random import shuffle\n",
        "# def shuffling_split(train_size, test_size, n_splits=1):\n",
        "#     total_size = train_size + test_size\n",
        "#     indices = [i for i in range(1,total_size+1)]\n",
        "#     train_indices = []\n",
        "#     test_indices = []\n",
        "#     for i in range(n_splits):\n",
        "#         indices = [i for i in range(1,total_size+1)]\n",
        "#         shuffle(indices)\n",
        "#         train = indices[:train_size]\n",
        "#         test = indices[train_size:]\n",
        "#         train_indices.append(train)\n",
        "#         test_indices.append(test)\n",
        "#     if n_splits == 1:\n",
        "#         return train_indices[0], test_indices[0]\n",
        "#     if n_splits > 1:\n",
        "#         return train_indices, test_indices\n",
        "\n",
        "# '_'.join('{}_{}'.format(key, val) for key, val in experiment.items())\n",
        "\n",
        "# labels = pd.DataFrame(dictlist)\n",
        "# test_indices = shuffling_split(experiment['trainSetSize'], experiment['valSetSize'])[1]\n",
        "# labels['TestSet'] = pd.to_numeric(labels['ImageIndex']).isin(test_indices)\n",
        "\n",
        "# data = (ImageList.from_df(labels, PATH + 'images/', cols='ImageName').split_from_df(col='TestSet').label_from_df(cols='ImageIndex').databunch(bs=48)).normalize(imagenet_stats)\n",
        "# ipath = Path(PATH + 'images/')\n",
        "\n",
        "# learn = cnn_learner(data, model_ids['alexnet'],  metrics=accuracy)\n",
        "\n",
        "# learn.summary()\n",
        "# learn.lr_find()\n",
        "# learn.recorder.plot()\n",
        "\n",
        "# class SaveFeatures():\n",
        "#     def __init__(self, module):\n",
        "#         self.hook = module.register_forward_hook(self.hook_fn)\n",
        "#     def hook_fn(self, module, input, output):\n",
        "#         self.features = output.clone().detach().requires_grad_(True).cuda()\n",
        "#     def close(self):\n",
        "#         self.hook.remove()\n",
        "        \n",
        "# def get_layer_names(layers):\n",
        "#     layer_names = []\n",
        "#     for layer in layers:\n",
        "#         layer_name = str(layer).split('(')[0]\n",
        "#         layer_names.append(layer_name + '-' + str(sum(layer_name in string for string in layer_names) + 1))\n",
        "#     return layer_names\n",
        "\n",
        "# def get_activations(model, img, layers, target_layer):\n",
        "#     img_tensor = preprocessing(img).unsqueeze(0).cuda()\n",
        "#     activations = SaveFeatures(layers[target_layer])\n",
        "#     model(img_tensor)\n",
        "#     activations.close()\n",
        "#     return activations.features.detach().cpu().numpy().squeeze()\n",
        "\n",
        "# def flatten_batch(features):\n",
        "#     return features.view(features.size()[0], torch.prod(torch.tensor(features.size()[1:])).item())\n",
        "\n",
        "# model = learn.model\n",
        "# layers = flatten_model(model)\n",
        "# layer_names = get_layer_names(layers)\n",
        "# model.eval()\n",
        "\n",
        "# classifier_rdms = {}\n",
        "# for layer_index, layer in enumerate(tqdm(layers[:-2])):\n",
        "#     image_activations = []\n",
        "#     for image in tqdm(labels[labels.TestSet == 1]['ImageName'][:100], leave=False):\n",
        "#         image_array = Image.open(ipath/image)\n",
        "#         image_activations.append(get_activations(model, image_array, layers, layer_index).flatten())\n",
        "#     layer_features = np.stack(image_activations)\n",
        "#     classifier_rdms[layer_names[layer_index]] = np.corrcoef(layer_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDzw6xquM6IX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(classifier_rdms)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
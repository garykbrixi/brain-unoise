{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mouse_data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1rnxfO73aJZyBrnUuKDsE7JInFxhDpFfG",
      "authorship_tag": "ABX9TyN4xyNu7WaU1yj4rXLFu9e0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/garykbrixi/brain-view/blob/master/mouse_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06NiuwR1YvoU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.utils.validation import check_is_fitted, check_array\n",
        "\n",
        "def svd_flip(u, v, u_based_decision=True):\n",
        "    u = u.view(u.size()[0],1)\n",
        "    v = v.view(v.size()[0],1)\n",
        "    if u_based_decision:\n",
        "        # columns of u, rows of v\n",
        "        max_abs_cols = torch.argmax(torch.abs(u), axis=0)\n",
        "        signs = torch.sign(u[list(max_abs_cols), range(u[0].size()[0])])\n",
        "        u *= signs\n",
        "        v *= signs.unsqueeze(1)\n",
        "    else:\n",
        "        # rows of v, columns of u\n",
        "        max_abs_rows = torch.argmax(torch.abs(v), axis=1)\n",
        "        signs = torch.sign(v[range(list(v.size())[0]), max_abs_rows])\n",
        "        u *= signs\n",
        "        v *= signs.unsqueeze(1)\n",
        "    u = u.flatten()\n",
        "    v = v.flatten()\n",
        "    return u, v\n",
        "\n",
        "def _nipals_twoblocks_inner_loop(X, Y, mode=\"A\", max_iter=500, tol=1e-06,\n",
        "                                 norm_y_weights=False):\n",
        "    \"\"\"Inner loop of the iterative NIPALS algorithm.\n",
        "    Provides an alternative to the svd(X'Y); returns the first left and right\n",
        "    singular vectors of X'Y.  See PLS for the meaning of the parameters.  It is\n",
        "    similar to the Power method for determining the eigenvectors and\n",
        "    eigenvalues of a X'Y.\n",
        "    \"\"\"\n",
        "\n",
        "    for col in Y.T:\n",
        "        if torch.any(torch.abs(col) > torch.finfo(torch.double).eps):\n",
        "\n",
        "            y_score = col.detach().view(col.size())\n",
        "\n",
        "            break\n",
        "\n",
        "    # for col in Y.T:\n",
        "    #     if np.any(np.abs(col) > np.finfo(np.double).eps):\n",
        "    #         y_score = col.reshape(len(col), 1)\n",
        "    #         break\n",
        "\n",
        "    x_weights_old = 0\n",
        "    ite = 1\n",
        "    X_pinv = Y_pinv = None\n",
        "    eps = torch.finfo(X.dtype).eps\n",
        "\n",
        "    if mode == \"B\":\n",
        "        # Uses condition from scipy<1.3 in pinv2 which was changed in\n",
        "        # https://github.com/scipy/scipy/pull/10067. In scipy 1.3, the\n",
        "        # condition was changed to depend on the largest singular value\n",
        "        X_t = X.dtype.char.lower()\n",
        "        Y_t = Y.dtype.char.lower()\n",
        "        factor = {'f': 1E3, 'd': 1E6}\n",
        "\n",
        "        cond_X = factor[X_t] * eps\n",
        "        cond_Y = factor[Y_t] * eps\n",
        "\n",
        "    # Inner loop of the Wold algo.\n",
        "    while True:\n",
        "        # 1.1 Update u: the X weights\n",
        "        if mode == \"B\":\n",
        "            if X_pinv is None:\n",
        "                # We use slower pinv2 (same as np.linalg.pinv) for stability\n",
        "                # reasons\n",
        "                X_pinv = torch.pinverse(X, check_finite=False, cond=cond_X)\n",
        "            x_weights = torch.mm(X_pinv, y_score)\n",
        "        else:  # mode\n",
        "            # Mode A regress each X column on y_score\n",
        "\n",
        "            x_weights = torch.mv(X.T, y_score) / torch.dot(y_score.T, y_score)\n",
        "        # If y_score only has zeros x_weights will only have zeros. In\n",
        "        # this case add an epsilon to converge to a more acceptable\n",
        "        # solution\n",
        "        if torch.dot(x_weights.T, x_weights) < eps:\n",
        "            x_weights += eps\n",
        "        # 1.2 Normalize u\n",
        "        x_weights /= torch.sqrt(torch.dot(x_weights.T, x_weights)) + eps\n",
        "        # 1.3 Update x_score: the X latent scores\n",
        "        x_score = torch.mv(X, x_weights)\n",
        "        # 2.1 Update y_weights\n",
        "        if mode == \"B\":\n",
        "            if Y_pinv is None:\n",
        "                # compute once pinv(Y)\n",
        "                Y_pinv = torch.pinverse(Y, check_finite=False, cond=cond_Y)\n",
        "            y_weights = torch.mm(Y_pinv, x_score)\n",
        "        else:\n",
        "            # Mode A regress each Y column on x_score\n",
        "            y_weights = torch.mv(Y.T, x_score) / torch.dot(x_score.T, x_score)\n",
        "        # 2.2 Normalize y_weights\n",
        "        if norm_y_weights:\n",
        "            y_weights /= torch.sqrt(torch.mm(y_weights.T, y_weights)) + eps\n",
        "        # 2.3 Update y_score: the Y latent scores\n",
        "        y_score = torch.mv(Y, y_weights) / (torch.dot(y_weights.T, y_weights) + eps)\n",
        "        # y_score = np.dot(Y, y_weights) / np.dot(y_score.T, y_score) ## BUG\n",
        "        x_weights_diff = x_weights - x_weights_old\n",
        "\n",
        "        if torch.dot(x_weights_diff.T, x_weights_diff) < tol or Y.size()[1] == 1:\n",
        "            break\n",
        "        if ite == max_iter:\n",
        "            warnings.warn('Maximum number of iterations reached',\n",
        "                          ConvergenceWarning)\n",
        "            break\n",
        "        x_weights_old = x_weights\n",
        "        ite += 1\n",
        "    return x_weights, y_weights, ite\n",
        "\n",
        "def _center_scale_xy(X, Y, scale=True):\n",
        "    \"\"\" Center X, Y and scale if the scale parameter==True\n",
        "    Returns\n",
        "    -------\n",
        "        X, Y, x_mean, y_mean, x_std, y_std\n",
        "    \"\"\"\n",
        "    # center\n",
        "    x_mean = torch.mean(X, axis=0)\n",
        "    X -= x_mean\n",
        "    y_mean = torch.mean(Y, axis=0)\n",
        "    Y -= y_mean\n",
        "    # scale\n",
        "    if scale:\n",
        "        x_std = torch.std(X, dim = 0)\n",
        "        x_std[x_std == 0.0] = 1.0\n",
        "        X = X/x_std\n",
        "        y_std = torch.std(Y, dim = 0)\n",
        "        y_std[y_std == 0.0] = 1.0\n",
        "        Y = Y/y_std\n",
        "    else:\n",
        "        x_std = torch.ones(X.size()[1])\n",
        "        y_std = torch.ones(Y.size()[1])\n",
        "    return X, Y, x_mean, y_mean, x_std, y_std\n",
        "\n",
        "class PLS:\n",
        "    def __init__(self, n_components=2, *, scale=True,\n",
        "                 deflation_mode=\"regression\",\n",
        "                 mode=\"A\", algorithm=\"nipals\", norm_y_weights=False,\n",
        "                 max_iter=500, tol=1e-06, copy=True):\n",
        "        self.n_components = n_components\n",
        "        self.deflation_mode = deflation_mode\n",
        "        self.mode = mode\n",
        "        self.norm_y_weights = norm_y_weights\n",
        "        self.scale = scale\n",
        "        self.algorithm = algorithm\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.copy = copy\n",
        "        self.scale = scale\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        \"\"\"Fit model to data.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Training vectors, where n_samples is the number of samples and\n",
        "            n_features is the number of predictors.\n",
        "        Y : array-like of shape (n_samples, n_targets)\n",
        "            Target vectors, where n_samples is the number of samples and\n",
        "            n_targets is the number of response variables.\n",
        "        \"\"\"\n",
        "        X = X.clone().detach()\n",
        "        Y = Y.clone().detach()\n",
        "        if Y.ndim == 1:\n",
        "            Y = Y.reshape(-1, 1)\n",
        "\n",
        "        n = X.size()[0]\n",
        "        p = X.size()[1]\n",
        "        q = Y.size()[1]\n",
        "\n",
        "        # if self.n_components < 1 or self.n_components > p:\n",
        "        #     raise ValueError('Invalid number of components: %d' %\n",
        "        #                      self.n_components)\n",
        "        # if self.algorithm not in (\"svd\", \"nipals\"):\n",
        "        #     raise ValueError(\"Got algorithm %s when only 'svd' \"\n",
        "        #                      \"and 'nipals' are known\" % self.algorithm)\n",
        "        # if self.algorithm == \"svd\" and self.mode == \"B\":\n",
        "        #     raise ValueError('Incompatible configuration: mode B is not '\n",
        "        #                      'implemented with svd algorithm')\n",
        "        # if self.deflation_mode not in [\"canonical\", \"regression\"]:\n",
        "        #     raise ValueError('The deflation mode is unknown')\n",
        "        # Scale (in place)\n",
        "        X, Y, self.x_mean_, self.y_mean_, self.x_std_, self.y_std_ = (\n",
        "            _center_scale_xy(X, Y, self.scale))\n",
        "        # Residuals (deflated) matrices\n",
        "        Xk = X\n",
        "        Yk = Y\n",
        "\n",
        "        # Results matrices\n",
        "        self.x_scores_ = torch.zeros((n, self.n_components))\n",
        "        self.y_scores_ = torch.zeros((n, self.n_components))\n",
        "        self.x_weights_ = torch.zeros((p, self.n_components))\n",
        "        self.y_weights_ = torch.zeros((q, self.n_components))\n",
        "        self.x_loadings_ = torch.zeros((p, self.n_components))\n",
        "        self.y_loadings_ = torch.zeros((q, self.n_components))\n",
        "        self.n_iter_ = []\n",
        "\n",
        "        # NIPALS algo: outer loop, over components\n",
        "        Y_eps = torch.finfo(Yk.dtype).eps\n",
        "\n",
        "        for k in range(self.n_components):\n",
        "            if torch.all(torch.mm(Yk.T, Yk) < torch.finfo(torch.double).eps):\n",
        "                # Yk constant\n",
        "                warnings.warn('Y residual constant at iteration %s' % k)\n",
        "                break\n",
        "            # 1) weights estimation (inner loop)\n",
        "            # -----------------------------------\n",
        "            if self.algorithm == \"nipals\":\n",
        "                # Replace columns that are all close to zero with zeros\n",
        "                Yk_mask = torch.all(torch.abs(Yk) < 10 * Y_eps, axis=0)\n",
        "                Yk[:, Yk_mask] = 0.0\n",
        "\n",
        "                x_weights, y_weights, n_iter_ = \\\n",
        "                    _nipals_twoblocks_inner_loop(\n",
        "                        X=Xk, Y=Yk, mode=self.mode, max_iter=self.max_iter,\n",
        "                        tol=self.tol, norm_y_weights=self.norm_y_weights)\n",
        "                self.n_iter_.append(n_iter_)\n",
        "\n",
        "            elif self.algorithm == \"svd\":\n",
        "                x_weights, y_weights = _svd_cross_product(X=Xk, Y=Yk)\n",
        "            # Forces sign stability of x_weights and y_weights\n",
        "            # Sign undeterminacy issue from svd if algorithm == \"svd\"\n",
        "            # and from platform dependent computation if algorithm == 'nipals'\n",
        "\n",
        "            # POTENTIAL TO DO\n",
        "\n",
        "            x_weights, y_weights = svd_flip(x_weights, y_weights.T)\n",
        "            y_weights = y_weights.T\n",
        "            # columns of u, rows of v\n",
        "\n",
        "            # compute scores\n",
        "            x_scores = torch.mv(Xk, x_weights)\n",
        "\n",
        "            if self.norm_y_weights:\n",
        "                y_ss = 1\n",
        "            else:\n",
        "                y_ss = torch.dot(y_weights.T, y_weights)\n",
        "\n",
        "            y_scores = torch.mv(Yk, y_weights) / y_ss\n",
        "\n",
        "            # test for null variance\n",
        "            if torch.dot(x_scores.T, x_scores) < torch.finfo(torch.double).eps:\n",
        "                warnings.warn('X scores are null at iteration %s' % k)\n",
        "                break\n",
        "            # 2) Deflation (in place)\n",
        "            # ----------------------\n",
        "            # Possible memory footprint reduction may done here: in order to\n",
        "            # avoid the allocation of a data chunk for the rank-one\n",
        "            # approximations matrix which is then subtracted to Xk, we suggest\n",
        "            # to perform a column-wise deflation.\n",
        "            #\n",
        "            # - regress Xk's on x_score\n",
        "\n",
        "            x_loadings = torch.mv(Xk.T, x_scores) / torch.dot(x_scores.T, x_scores)\n",
        "            # - subtract rank-one approximations to obtain remainder matrix\n",
        "            Xk -= x_scores[:, None] * x_loadings.T\n",
        "            if self.deflation_mode == \"canonical\":\n",
        "                # - regress Yk's on y_score, then subtract rank-one approx.\n",
        "                y_loadings = (torch.mv(Yk.T, y_scores)\n",
        "                              / torch.dot(y_scores.T, y_scores))\n",
        "                Yk -= y_scores[:, None] * y_loadings.T\n",
        "            if self.deflation_mode == \"regression\":\n",
        "                # - regress Yk's on x_score, then subtract rank-one approx.\n",
        "                y_loadings = (torch.mv(Yk.T, x_scores)\n",
        "                              / torch.dot(x_scores.T, x_scores))\n",
        "                Yk -= x_scores[:, None] * y_loadings.T\n",
        "            # 3) Store weights, scores and loadings # Notation:\n",
        "            self.x_scores_[:, k] = x_scores.view(-1)  # T\n",
        "            self.y_scores_[:, k] = y_scores.view(-1)  # U\n",
        "            self.x_weights_[:, k] = x_weights.view(-1)  # W\n",
        "            self.y_weights_[:, k] = y_weights.view(-1)  # C\n",
        "            self.x_loadings_[:, k] = x_loadings.view(-1)  # P\n",
        "            self.y_loadings_[:, k] = y_loadings.view(-1)  # Q\n",
        "\n",
        "        # Such that: X = TP' + Err and Y = UQ' + Err\n",
        "\n",
        "        # 4) rotations from input space to transformed space (scores)\n",
        "        # T = X W(P'W)^-1 = XW* (W* : p x k matrix)\n",
        "        # U = Y C(Q'C)^-1 = YC* (W* : q x k matrix)\n",
        "        self.x_rotations_ = torch.mm(\n",
        "            self.x_weights_,\n",
        "            torch.pinverse(torch.mm(self.x_loadings_.T, self.x_weights_)))\n",
        "        if Y.size()[1] > 1:\n",
        "            self.y_rotations_ = torch.mm(\n",
        "                self.y_weights_,\n",
        "                torch.pinverse(torch.mm(self.y_loadings_.T, self.y_weights_)))\n",
        "        else:\n",
        "            self.y_rotations_ = torch.ones(1)\n",
        "\n",
        "        if True or self.deflation_mode == \"regression\":\n",
        "            # FIXME what's with the if?\n",
        "            # Estimate regression coefficient\n",
        "            # Regress Y on T\n",
        "            # Y = TQ' + Err,\n",
        "            # Then express in function of X\n",
        "            # Y = X W(P'W)^-1Q' + Err = XB + Err\n",
        "            # => B = W*Q' (p x q)\n",
        "\n",
        "            self.coef_ = torch.mm(self.x_rotations_, self.y_loadings_.T)\n",
        "            self.coef_ = self.coef_.to(\"cuda\")\n",
        "            self.y_std_ = self.y_std_.to(\"cuda\")\n",
        "            # self.coef_ = torch.mv(self.coef_, self.y_std_)\n",
        "            self.coef_ = self.coef_[:, None] * self.y_std_\n",
        "            self.coef_ = self.coef_[:,0,:]\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, Y=None, copy=True):\n",
        "        \"\"\"Apply the dimension reduction learned on the train data.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Training vectors, where n_samples is the number of samples and\n",
        "            n_features is the number of predictors.\n",
        "        Y : array-like of shape (n_samples, n_targets)\n",
        "            Target vectors, where n_samples is the number of samples and\n",
        "            n_targets is the number of response variables.\n",
        "        copy : boolean, default True\n",
        "            Whether to copy X and Y, or perform in-place normalization.\n",
        "        Returns\n",
        "        -------\n",
        "        x_scores if Y is not given, (x_scores, y_scores) otherwise.\n",
        "        \"\"\"\n",
        "        check_is_fitted(self)\n",
        "        X = check_array(X, copy=copy, dtype=FLOAT_DTYPES)\n",
        "        # Normalize\n",
        "        X -= self.x_mean_\n",
        "        X /= self.x_std_\n",
        "        # Apply rotation\n",
        "        x_scores = torch.mm(X, self.x_rotations_)\n",
        "        if Y is not None:\n",
        "            Y = check_array(Y, ensure_2d=False, copy=copy, dtype=FLOAT_DTYPES)\n",
        "            if Y.ndim == 1:\n",
        "                Y = Y.reshape(-1, 1)\n",
        "            Y -= self.y_mean_\n",
        "            Y /= self.y_std_\n",
        "            y_scores = torch.mm(Y, self.y_rotations_)\n",
        "            return x_scores, y_scores\n",
        "\n",
        "        return x_scores\n",
        "\n",
        "    def inverse_transform(self, X):\n",
        "        \"\"\"Transform data back to its original space.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape (n_samples, n_components)\n",
        "            New data, where n_samples is the number of samples\n",
        "            and n_components is the number of pls components.\n",
        "        Returns\n",
        "        -------\n",
        "        x_reconstructed : array-like of shape (n_samples, n_features)\n",
        "        Notes\n",
        "        -----\n",
        "        This transformation will only be exact if n_components=n_features\n",
        "        \"\"\"\n",
        "        check_is_fitted(self)\n",
        "        X = check_array(X, dtype=FLOAT_DTYPES)\n",
        "        # From pls space to original space\n",
        "        X_reconstructed = torch.matmul(X, self.x_loadings_.T)\n",
        "\n",
        "        # Denormalize\n",
        "        X_reconstructed *= self.x_std_\n",
        "        X_reconstructed += self.x_mean_\n",
        "        return X_reconstructed\n",
        "\n",
        "    def predict(self, X, copy=True):\n",
        "        \"\"\"Apply the dimension reduction learned on the train data.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Training vectors, where n_samples is the number of samples and\n",
        "            n_features is the number of predictors.\n",
        "        copy : boolean, default True\n",
        "            Whether to copy X and Y, or perform in-place normalization.\n",
        "        Notes\n",
        "        -----\n",
        "        This call requires the estimation of a p x q matrix, which may\n",
        "        be an issue in high dimensional space.\n",
        "        \"\"\"\n",
        "        # TODO: check fitted and check array\n",
        "        # check_is_fitted(self)\n",
        "        # X = check_array(X, copy=copy, dtype=FLOAT_DTYPES)\n",
        "        # Normalize\n",
        "        X -= self.x_mean_\n",
        "        X /= self.x_std_\n",
        "        # print(X[:, None] * self.coef_ + self.y_mean_)\n",
        "        # print(torch.mv(X, self.coef_) + self.y_mean_)\n",
        "        Ypred = torch.mm(X, self.coef_)\n",
        "        return Ypred + self.y_mean_\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        \"\"\"Learn and apply the dimension reduction on the train data.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Training vectors, where n_samples is the number of samples and\n",
        "            n_features is the number of predictors.\n",
        "        y : array-like of shape (n_samples, n_targets)\n",
        "            Target vectors, where n_samples is the number of samples and\n",
        "            n_targets is the number of response variables.\n",
        "        Returns\n",
        "        -------\n",
        "        x_scores if Y is not given, (x_scores, y_scores) otherwise.\n",
        "        \"\"\"\n",
        "        return self.fit(X, y).transform(X, y)\n",
        "\n",
        "    def _more_tags(self):\n",
        "        return {'poor_score': True}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoQq6f9vhNV6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "c04ad555-075e-4a23-a60c-9fe93993b453"
      },
      "source": [
        "import pickle\n",
        "# drive.mount('/drive/My Drive')\n",
        "PATH = \"drive/My Drive/neuro140/\"\n",
        "infile = open(PATH+'/mouse_brain_data_sample.pkl','rb')\n",
        "mouse_pickle = pickle.load(infile)\n",
        "VISam = mouse_pickle['VISam']\n",
        "VIspm = mouse_pickle['VISpm']\n",
        "\n",
        "print(VIspm.shape)\n",
        "\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(VISam, VIspm, test_size=0.2, random_state=42)\n",
        "\n",
        "R_X = torch.tensor(X_train).type(torch.cuda.FloatTensor)\n",
        "R_Y = torch.tensor(Y_train).type(torch.cuda.FloatTensor)\n",
        "R_Xtest = torch.tensor(X_test).type(torch.cuda.FloatTensor)\n",
        "modl = PLS(n_components=25).fit(R_X, R_Y)\n",
        "y_pred = modl.predict(R_Xtest)\n",
        "y_pred = y_pred.cpu().numpy()\n",
        "print(np.average((1 + Y_test - y_pred)/ (1 + y_pred)))\n",
        "\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_test.shape)\n",
        "\n",
        "modl = PLSRegression(n_components=25).fit(VISam, VIspm)\n",
        "modl.fit(X_train, Y_train)\n",
        "y_pred = modl.predict(X_test)\n",
        "print(np.average((1 + Y_test - y_pred)/ (1 + y_pred)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(119, 571)\n",
            "0.9890784382500725\n",
            "(95, 221)\n",
            "(95, 571)\n",
            "(24, 221)\n",
            "(24, 571)\n",
            "0.9890782905903662\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhqulAaCZdqV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "88ba43a3-d5cc-4ad0-d2d0-8a204ccec759"
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "# from fastai.torch_core import flatten_model\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "from fastai.vision import *\n",
        "from fastai.metrics import error_rate\n",
        "\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "model_ids = {'vgg16': models.vgg16_bn, 'vgg19': models.vgg19_bn, 'resnet18': models.resnet18, 'resnet34': models.resnet34, 'resnet50': models.resnet50, 'resnet152': models.resnet152}\n",
        "\n",
        "experiment = {'train_data': 'SimpleShapes',\n",
        "              'test_data': 'SimpleShapes',\n",
        "              'subset': 'Stack_3',\n",
        "              'model': 'alexnet', \n",
        "              'finetuning': False, \n",
        "              'trainCycles': 3, \n",
        "              'trainSetSize': 800,\n",
        "              'valSetSize': 200}\n",
        "'_'.join('{}_{}'.format(key, val) for key, val in experiment.items())\n",
        "\n",
        "learn = cnn_learner(data, model_ids[experiment['model']],  metrics=accuracy)\n",
        "\n",
        "preprocessing = transforms.Compose([\n",
        "    transforms.Resize((224,224)), \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "class SaveFeatures():\n",
        "    def __init__(self, module):\n",
        "        self.hook = module.register_forward_hook(self.hook_fn)\n",
        "    def hook_fn(self, module, input, output):\n",
        "        self.features = output.clone().detach().requires_grad_(True).cuda()\n",
        "    def close(self):\n",
        "        self.hook.remove()\n",
        "        \n",
        "def get_layer_names(layers):\n",
        "    layer_names = []\n",
        "    for layer in layers:\n",
        "        layer_name = str(layer).split('(')[0]\n",
        "        layer_names.append(layer_name + '-' + str(sum(layer_name in string for string in layer_names) + 1))\n",
        "    return layer_names\n",
        "\n",
        "def get_activations(model, img, layers, target_layer):\n",
        "    img_tensor = preprocessing(img).unsqueeze(0).cuda()\n",
        "    activations = SaveFeatures(layers[target_layer])\n",
        "    model(img_tensor)\n",
        "    activations.close()\n",
        "    return activations.features.detach().cpu().numpy().squeeze()\n",
        "\n",
        "def flatten_batch(features):\n",
        "    return features.view(features.size()[0], torch.prod(torch.tensor(features.size()[1:])).item())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-3bb8f259031b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m'_'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m preprocessing = transforms.Compose([\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    }
  ]
}